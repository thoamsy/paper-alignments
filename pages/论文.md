# 面向小规模量的全文搜索引擎的设计和实现

# 摘要

时代飞速发展, 技术日新月异, 对开发者的要求也越来越高. 当年的“切图仔”到现在的**全栈工程师, 大前端**就是前端工程师的发展写照.
那些认为女生才应该去写前端的, 前端就是写写界面的人, 在目前看来可以说是错的离谱.
最基本的前端工程师需要了解的知识体系甚至比后端还复杂. 每一个前端工程师都需要会写服务器代码, 每一个前端工程师都需要会写原生应用.

好在 JavaScript 确实赋予了他们这种能力. Nodejs 是让前端开发者抬起头, 后端开发者学习前端的一个跨时代的平台. React Native 的出现让 JavaScript 也能写 iOS 和 Android 应用. Electron 更是让 JavaScript 可以攻占桌面级的应用. 本论文编写的所有代码都是由一个基于 JavaScript 搭建的编辑器完成.

可以说, JavaScript 的生态已经势不可挡了. 所以本论文也希望通过搭上这本快车, 使用 Js 来投建一个简易的搜索引擎, 来探索**全栈的可能性**. 其中前端采用的是 React, 后端由 Koa 搭建.

该搜索引擎会爬取 Udacity 的一些内容, 尝试在输入关键字后, 能够将本站的相关网页视频显示出来. 排名的方式主要基于其他网页索引该网站的次数.

关键字: React; Koa; nodejs; Search Engine;

# 绪论

## 发展意义和背景

随着时代的不断演进, 人们对于互联网的依赖程度, 以及对**用户体验**要求体检提升, 开发人员面临的挑战也越来越多. 知识和技术已经远远不够学, 作者可能刚将一本技术书写完, 该技术就更新了新版本, 带来了**破坏性改变**, 一夜之间这本书似乎就过时了. 为了更快速的学习知识, 搜索引擎对于开发人员来说是不可或缺的. 面向 Google 编程, 面向 Stack Overflow 编程是一种趋势. 如果没有 Google, 开发人员很难找到解决方案. Stack Overflow 里如果不能搜索, 这个网站也没有存在的意义. 甚至, 如果 bilibili 不能搜索的话, 它也不能成为如此强势的视频网站.
所以, 尝试自己去搭建一个搜索引擎, 是十分必要的. 再也不用惊叹于搜索的神奇, 拉近和后端工程师们的距离.

## 发展趋势

国内: 国内的搜索引擎以百度最为闻名. 但是因为过于商业化, 以及对技术人员的不友好, 基本上就和 Siri 一样, 总是答非所问. 在国内一般是作为不能科学上网的备用选择. 而其他的搜索引擎, 也就是某数字公司和某动物公司的搜索引擎有一定的市场份额, 但基本没有尝试的必要国外: 国外的著名的搜索引擎有: **Google, Bing, Yahoo, DuckDuckGo**. Google 和 DuckDuckGo 的英文体验相当, 都能给出想要的效果, 而且不会上传用户的隐私. 它们都支持自动翻译和错别字矫正功能.
但是近年来, 随着 Google 向人工智能的发力, 我们用 Google 搜索的任何内容都会被分析, 并生成一个用户的特征模型, 以能够推送更加准确的信息流. 对, 现在 Google 不仅仅是一个搜索引擎, 而是一套信息服务. 本论文显然不会以超越 Google 为目标, 它最大的功劳是协助这篇论文的面世.

## 系统开发目标

前端采用 React 构建, React 和传统 MVC 模式的框架不同, 采用的是一种声明式的编程方式. 将**函数式编程的思想渗透其中**. 后端采用的 Koa 是一个比较新的 Nodejs 框架. 它充分拥抱了 `async await` 模式, 从 nodejs 的 `callback hell` 中解脱出来, 让开发者能够使用更为现代的方式编程.
搜索引擎的目标是在确定搜索后, 顺序的响应的用户的搜索内容, 并给出一个拥有合理排序的搜索内容.

# 基本背景知识

## HTTP 协议

为了更好的理解搜索引擎的本质和原理, 先来一起探索下它所依赖的协议: **HTTP.**
HTTP 最新版本是 2, 相比 1.1 带来革命性的提升和改变. 不过为了方便理解, 本文还是基于 HTTP 1.1 来解释整个流程. 另外, HTTP 协议以下的协议栈, 只解释到 TCP 层.

拿 Google 举例, 在搜索框中输入搜索内容后, 按下回车或者 **Search** 按钮, 开始的第一步就是将输入的内容包传成一个叫做 **q** 的参数, 以及其他细节参数一起组装好. 加上 HTTP 请求行, HTTP 首部. 包装成一个 **GET** 请求. 同样的, 为了简单, 这里还是忽略了很多细节, 比如 Google 的搜索建议.

这个请求可能长这样:

```
GET / HTTP/1.1
HOST: www.google.com.hk
Accept: */*
```

但是包装好了, 还需要一个伙计将该请求传送到 google 的 IP 下. 关于获取 IP 的方法不是本文的重点, 只要知道是通过 **DNS** 协议来实现的就行. 一旦获取到 IP, 这个伙计就能工作了. 欢迎 **TCP**

TCP 协议最广为人知的地方就是它的三次握手, 简单回顾下.

1.  浏览器发出一段 datagram 给 Google , 里面包含了 SYN 字段
2.  Google 收到该 datagram 后, 作出处理(比如分配内存), 也发出一段 datagram, 包含了 SYN 和 ACK 字段
3.  浏览器收到 datagram, 将之前的生成的 HTTP 请求和 ACK 字段一起发送给 Google

经过这些步骤, Google 服务器终于知道要搜索什么内容, 并做了一番处理, 将结果返回给了浏览器. 这里的一番处理可不简单, 也是本文讨论的重点. 不过先讲它留在后面, 继续看看之后的步骤.

浏览器获取到了 Google 的响应, 比如这样

```
HTTP/1.1 200 OK
content-type: text/html
content-length: …

…………
```

Google 返回的仅仅是一个 HTML 文件. 随着浏览器去解析该 HTML 文件, 接着就会将 HTML 中的其他**关键路径上的文件**获取. 比如 CSS, JS 或者其他 SVG 图片. 最后就能看到结果.

## TLS

上面其实遗漏了一些内容. Google 现在是基于 HTTPS 的, 也就是说, 在发送 HTTP 请求之前, 还需要和 TLS 协议进行一段沟通. 一起探索下

1.  浏览器发送一段 _Client Hello_ 信息给 Google, 并且带上了一个随机数和支持的加密方法
2.  Google 回应了一段 _Server Hello_ 信息给浏览器, 带上了一个随机数
3.  Google 继续行动. 把它的证书发送给浏览器(在某些金融网点, 还需要浏览器提供证书). 这是一个 _Server Done_ 信息
4.  浏览器确认 Google 的证书有效, 就会生成第三个随机数, **Premaster secret**. 用 Google 的公钥加密这个随机数并发送给 Google. 如果发现证书无效或者过期了, 浏览器就会发出警告, 阻止用户访问
5.  Google 使用自己的私钥解密了这个浏览器. 这个时候, Google 和浏览器都会使用这三个随机数生成的一个会话秘钥**session key** 来加密接下来的信息
6.  浏览器会发送一个 _Change cipher spec_ 来通知 Google, 接下来所有的信息都要开始加密啦. 同时发送 _Client Finished_ 信息
7.  Google 收到了该通知, 开始使用会话秘钥. 并发送 _Server Finished_ 信息
8.  现在浏览器和 Google 已经建立好加密通道, 所有的消息都会被加密. 接下来的步骤就回到了上面的 TCP 过程

这个过程却是很复杂, 至于公钥和私钥的生成细节这里就忽略, 以便带来更多的复杂性.
虽然本文跳过了一些步骤, 但是主要的过程已经描述清楚. 可以看一张图来巩固一下
![1_rjBdCBwOx5Gp_A6b6FQgfw](/Users/yk/Documents/paper/pages/1_rjBdCBwOx5Gp_A6b6FQgfw.png)

回到 Google 返回 HTML 的地方, 浏览器到底是如何使用 HTML, CSS, JavaScript 在屏幕上渲染漂亮的网页呢?

## 关键渲染路径

一个 HTML 文件从加载到显示出来的过程, 被称为 **关键渲染路径**. 一般分为 4 个步骤

1.  DOM 树构建
2.  CSSOM 构建
3.  布局(Layout)
4.  绘制(Paint)
    本文暂且不讨论 JavaScript 的逻辑. 因为现代 JavaScript 过于复杂, 这方面有很优秀的[素材](https://developers.google.cn/web/fundamentals/performance/critical-rendering-path/adding-interactivity-with-javascript)

### DOM 树的构建

为了简单复杂性, 先假设 HTML 里仅仅包含一些简单的文本和一张图片, 则 DOM 树的构建过程为

1.  **转换:** 浏览器先读区磁盘(缓存)或者网络中 HTML 的原始字节, 使用文件指定的编码方式(UTF-8), 将它们转化成字符
2.  **Token 化:** 将字符中所有 [W3C 标准中规定的标签](http://www.w3.org/TR/html5/)换成特定的 Token
3.  **词法构建:** 将所有 Token 转化成规则对象
4.  **DOM 构建:** 根据标签和标签之间的包含关系来建立 DOM 树

![DOM 构建](/Users/yk/Documents/paper/pages/dom-tree.png)

整个流程还可以用一张图来表示
![整个流程](/Users/yk/Documents/paper/pages/full-process.png)

### CSSOM 构建

当然, 在构建 DOM 树的构成中, 会碰到 `link` 标签. 如果标签中引用了 css 文件的话, 浏览器**优先去**构建 CSSOM, 不过过程和构建 DOM 树相似.

### 渲染树

在 DOM 和 CSSOM 都构建好后, 浏览器就会将 DOM 和 CSSOM 合并, 生成新的 **Render Tree**
为了构建它, 浏览器做了这些步骤:

1.  从 DOM 树根节点开始遍历不可见的节点: `meta, script`, 不会出现在渲染树中通过 CSS 隐藏的节点: `display: none`, 也不会出现在渲染树中
2.  对所有可见的节点, 应用 CSSOM 的规则
3.  生成渲染树

![渲染树](/Users/yk/Documents/paper/pages/render-tree-construction.png)

接着根据渲染树中的样式信息, 浏览器进入 **Layout** 阶段. 这个阶段是为了弄清每个对象的确切位置和大小.
最后根据位置和大小, 浏览器就能将它们写入实际的像素中, 这里被称为 **Paint**

经历“穷山恶水”, 浏览器终于成功渲染了这些文字和图片.

# 搜索引擎

了解了基本的浏览器的运行机制, 现在可以进入到正题. 搜索引擎的构建, 在本文中, 主要分为三步:

1.  构建爬虫
2.  使用爬取的数据构建索引
3.  寻找一个合理的 Rank 算法

先从构建爬虫开始.
作为一个个人项目, 不可能也不应该尝试去像其他搜索引擎一样, 爬取所有互联网上的页面, 这对于我们来说是一个不可能完成的任务. 应该将爬虫放在某个特定的网站下面, 本文使用的是 [Udacity](https://www.udacity.com), 作为 **种子** , 也就是爬虫开始的第一个界面.

## 获取一个网页上的超链接

为了测试爬虫的可行性, 先将任务简化, 从只获取一个网页的所有 **href** 开始, 不做任何文本处理. 至于 href 的定义, 或者说 _Hyper Reference_, 不是需要被下载的链接的意思. 比如, **img, css, JavaScript** 这些文件都需要被下载, 但显然不是爬虫感兴趣的内容. 现在只讨论 HTML 和 txt 文件, 一般情况下它们都是出现在 a 标签的 href 属性里, 换句话说, 只要把一个网页中所有的 href 都找到就找到了所有的 href.
好在根据 [CSS Selector](https://www.w3schools.com/cssref/css_selectors.asp) 规范, 使用一个很简单的 CSS 选择器就能实现 `a[href]`.

不过等等, 爬虫是基于服务器建立的, CSS 选择器属于 W3C 标准, 但是好像只是各大浏览器实现了这个功能, 怎么运用在后端呢? 有一个问题需要澄清下, **标准是标准, 实现是实现. 实现可以参考标准, 并在任何平台实现**. 所以说, 浏览器上的 CSS 选择器是浏览器的实现, 后端当然也有后端的实现, 比如本文将要采用到的 [cheerio](http://cheerio.js.org), 就是 nodejs 的一种**实现**

Nodejs 包管理的方法本文不讨论, 只要下载好后,
`import cheerio from 'cheerio'`
就能开箱使用了.
而想要获取一个网页的所有 href 内容的话, 可以写成

```js
const text = '………'; // text 变量是一长串文本
const $ = cheerio(text);

const allLinks = $('a[href]').map((_, element) => $(element).attr('href'))).get()
```

通过这种方式, 就能获得 text 所代表的 HTML 中所有的 href. 但是这似乎没什么用, 传入的参数应该是 URL, 而不是整个 HTML 文件. 毕竟一个爬虫, 应该只需要关注种子网页就行了.
OK, 这也不是难事, 只不过多一步 fetch URL 所拥有的 HTML 文件内容, 并将它作为上面的 text 就行了.
在 node 里, 可以这么实现

```js
const url = 'https://www.google.com';
const getPage = async url => {
  try {
    const html = await fetch(url).then(res => res.text());
    return cheerio.load(html);
  } catch (err) {
    console.log(err);
  }
};
const $ = getPage(url);
```

**再次重申, 本文不讨论任何和语法有关的话题.**
Fetch URL 的操作, 就由第四行, 也就是含有 `fetch` 的那一行完成了这个任务.
这样的话, 获取所有链接的任务就和之前的代码一样, 因为此时的 `$` 已经获得了 URL 的 HTML.

这段代码看上去没有任何问题, 但是, 现在还有一个隐藏的问题需要指出来.
`fetch` 只能支持**绝对路径**的 URL!
这能理解, 给一个 \_.\_abc.html/ 这种链接, 还不如使用文件读取方法直接读取.
但是什么时候会碰到相对路径的情况呢?? 乍一看很难理解, 因为我们可以控制种子网页的内容, **不可能会传一个相对路径的 URL 进去**. 当然, 目前不会出现问题. 可是别忘了爬虫的目的, 不就是将当前网页的所有 href 获取后, 重复当前过程吗? href 是很可能包含的是相当路径!!!
这个问题有两种方法来解决, 第一种是过滤所有的相对路径, 第二种则是将相对路径转化成绝对路径. 第一种更加简单, 只需要改一下选择器, 改为 `a[href^=http]` 就解决问题了. 这个选择器的意思就是**筛选所有 href 属性由 http 开头的 a 标签**, 这样就能保证得到的路径都是绝对路径.
当然, 作为开发人员的角度, 相对路径比绝对路径好写多了. 所有一个网站中, 更多的可能使用的还是相对路径, 这种自损三千的方法, 接下来会改进.

## 放出爬虫

现在已经知道如何获得一个网页中所有的 href, 可以使用下面的代码来看看效果

```js
console.log(allLinks(getPage('https://www.udacity.com')));
```

不出意外的话, 就能看到下面的效果
![输出结果](/Users/yk/Documents/paper/pages/6FFEB29B-65B0-4131-BDAB-5EFEEEC478B2.png)
接下来可以将爬取到的所有链接利用起来. 将爬取到的所有网页内容, 加入数组, 递归执行爬取步骤.
已经是递归, 就需要有一个递归的终止条件. 这里的终止条件就是数组为空为止.
这里会引发一个问题, 数组会为空吗? 我们的答案是很可能不会.
原因有几点:

1.  存在外链
2.  链接之间会相互引用
3.  链接太多

最后一个很容易理解, 如果链接太多的话, 那么想要在相对短的时间内爬取所有网页显然是不可能的. 一个可行的方法是规定爬取次数否则爬取时间. 比如爬取 10000 个或者运行十分钟后停止.
前面的两个原因, 也能很好的解决. 第一个问题我们只要对爬取到的链接进行一次筛选, 将主机不是 Udacity 的域名去掉. 而第二个循环链接的问题, 只需要保存所有访问过的链接, 并且只访问没有访问过得链接.
其实很容易发现, 循环链接的问题和广度优先搜索的解决方法很相似. 没错, 爬虫使用的就是广度优先的策略. 为什么不用深度优先呢? 因为这很容易导致栈溢出…

通过上面的分析, 爬虫终于可以派上用场了.

```js
const crawlWeb = async seed => {
  let tocrawl = [seed];
  const crawled = {};

  while (tocrawl.length) {
    const page = tocrawl.pop();
    if (!crawled[page]) {
      const $ = await getPage(page);
      const outlinks = getAllLinks($);
      tocrawl = union(tocrawl, outlinks).filter(link => {
        return new URL(link).hostname.includes('Udacity');
      });
      crawled[page] = true;
    }
  }
};
```

接下来要解决之前遗留的问题: **处理相对路径**. 如何判断一个路径时候是相对路径这没什么疑问, 但是可以采取一个特殊的方式. Nodejs 的 URL 方法只能转换绝对路径, 否则就会抛出异常.
比如 `new URL('./abc')` 这样就会报错. 于是可以利用它的报错, 配合 `try catch` 来将相对路径转化成绝对路径. 回到之前 `getAllLinks`

```js
const getAllLinks = ($, page) => {
  const { origin } = new URL(page);
  return $('a[href]')
    .map((_, element) => {
      const href = $(element).attr('href');
      try {
        new URL(href);
      } catch (err) {
        if (href[0] === '/') {
          return `${origin}${href}`;
        }
        return `${origin}/${href}`;
      }
      return href;
    })
    .get();
};
```

`getAllLinks` 和之前相比多了一参数—page, 它用来保存正在爬取的网页的信息. 可以通过它了解到当前网站的 hostname 和 origin, 再添加到相对路径之前就可以得到绝对路径.

### robot.txt

但是爬虫并不是肆无忌惮的. 一般网站都会有一个 [robot.txt](https://en.wikipedia.org/wiki/Robots_exclusion_standard) 文件, 来告诉爬虫: 哪些路径是运行爬取的, 而哪些路径是不运行爬取的.
爬虫必须遵守它的规则, 否则本地的 IP 很容易被网站加入黑名单, 再也不能访问该网站. 所以我们的爬取策略也要做出修改, 在开始爬取之前必须先读取站点的 robot 文件.
这个文件的位置很有讲究, 一般都位于网站的根目录下. 比如 Udacity 的 robot 地址应该是 `www.udacity.com/robot.txt`.

Udacity 的 robot 长这样

![Udacity robot.text](/Users/yk/Documents/paper/pages/F5FB8051-0F17-4945-BB89-C9F2FC6FFFE2.png)

相对来说是很简单的, 针对这种简单形式的 robot, 可以实现一个自己的解析函数

```js
const fetchRobot = async url => {
  const robotUrl = `${url}/robots.txt`;
  const disallow = [];
  if (robotUrl) {
    const robot = await fetch(robotUrl).then(res => res.text());
    robot
      .split('\n')
      .filter(str => str.startsWith('Disallow'))
      .forEach(ban => disallow.push(ban.slice(ban.indexOf('/'))));
  }
  return disallow;
};
```

因为结构的单一, 只需要取出所有以 `Disallow` 开头的行的链接, 得到一个数组. 因为这个数组也不长, 在检测时候是合法链接的时候, 可以尝试一种十分低效但是简单的方式: 对于爬取的每个链接都在这个数组中检查一下.
